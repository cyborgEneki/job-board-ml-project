{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "base_url = 'https://opportunitiesforyoungkenyans.co.ke'\n",
    "pagination_date_pattern = base_url + '/2024/06/13'\n",
    "pagination_pattern = pagination_date_pattern + '/page/3'\n",
    "\n",
    "headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_body(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_article = section.find('article', class_='post-content')\n",
    "\n",
    "      if post_article:\n",
    "        post_container = post_article.find('div', class_='clearfix').text\n",
    "        return post_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_url(url):\n",
    "  parsed_url =  urlparse(url)\n",
    "  \n",
    "  path_parts = parsed_url.path.split('/')\n",
    "  \n",
    "  year = path_parts[1]\n",
    "  month = path_parts[2]\n",
    "  day = path_parts[3]\n",
    "  \n",
    "  date = f'{year}-{month}-{day}'\n",
    "  return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_postings(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_container = section.find('div', class_='col-lg-8')\n",
    "\n",
    "      job_posts = []\n",
    "      \n",
    "      for post in post_container.find_all('div', class_='post-classic'):\n",
    "        title_heading_element = post.find('h5')\n",
    "\n",
    "        title = title_heading_element.find('a').text\n",
    "        link = title_heading_element.find('a').get(\"href\")\n",
    "\n",
    "        content = get_post_body(link, headers)\n",
    "        date = extract_date_from_url(link)\n",
    "\n",
    "        job_posts.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          'content': content\n",
    "        })\n",
    "\n",
    "      return job_posts\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "all_jobs = get_job_postings(pagination_date_pattern, headers)\n",
    "df = pd.DataFrame(all_jobs)\n",
    "df.to_csv('job_postings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
