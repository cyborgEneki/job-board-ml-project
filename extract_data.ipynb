{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from datetime import date\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Normalize Unicode characters\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_body(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_article = section.find('article', class_='post-content')\n",
    "      if post_article:\n",
    "        # post_container = post_article.find('div', class_='clearfix').text\n",
    "        post_container = post_article.find('div', class_='clearfix')\n",
    "        \n",
    "        if post_container:\n",
    "          post_container_text =  clean_text(post_container.get_text(separator=\" \", strip=True))\n",
    "\n",
    "          if len(post_container_text) > 10000:\n",
    "            post_container_text = len(post_container_text)\n",
    "            \n",
    "          return post_container_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_url(url):\n",
    "  parsed_url =  urlparse(url)\n",
    "  \n",
    "  path_parts = parsed_url.path.split('/')\n",
    "  \n",
    "  year = path_parts[1]\n",
    "  month = path_parts[2]\n",
    "  day = path_parts[3]\n",
    "  \n",
    "  date = f'{year}-{month}-{day}'\n",
    "  return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_postings(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_container = section.find('div', class_='col-lg-8')\n",
    "\n",
    "      job_posts = []\n",
    "      \n",
    "      for post in post_container.find_all('div', class_='post-classic'):\n",
    "        title_heading_element = post.find('h5')\n",
    "\n",
    "        title = clean_text(title_heading_element.find('a').text.strip())\n",
    "        link = title_heading_element.find('a').get(\"href\")\n",
    "\n",
    "        content = get_post_body(link, headers)\n",
    "        date = extract_date_from_url(link)\n",
    "\n",
    "        job_posts.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          'content': content,\n",
    "          'date': date\n",
    "        })\n",
    "      return job_posts\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "# all_jobs = get_job_postings(pagination_date_pattern, headers)\n",
    "# df = pd.DataFrame(all_jobs)\n",
    "# df.to_csv('job_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_job_postings():\n",
    "  headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "  }\n",
    "\n",
    "  first_year = 2018\n",
    "  first_month = 1\n",
    "\n",
    "  date_today = date.today()\n",
    "  year_today = date_today.year\n",
    "  month_today = date_today.month\n",
    "\n",
    "  base_url = 'https://opportunitiesforyoungkenyans.co.ke'\n",
    "\n",
    "  jobs_from_all_dates = []\n",
    "  merged_jobs = []\n",
    "\n",
    "  for year in range(first_year, year_today + 1):\n",
    "    if year == year_today:\n",
    "      last_month = month_today\n",
    "    else:\n",
    "      last_month = 12\n",
    "    \n",
    "    for month in range(first_month, last_month + 1):\n",
    "      current_month_url = f'{base_url}/{year}/{month}'\n",
    "      current_month_url_response = requests.get(current_month_url, headers=headers)\n",
    "      print(current_month_url)\n",
    "      if current_month_url_response.status_code == 200:\n",
    "        for page_count in range(1, 1000):\n",
    "          page_url = f'{current_month_url}/page/{page_count}'\n",
    "          page_url_response = requests.get(page_url, headers=headers)\n",
    "          \n",
    "          if page_url_response.status_code == 200:\n",
    "            page_url_soup = BeautifulSoup(page_url_response.content, 'html.parser')\n",
    "            page_url_body_tag = page_url_soup.body\n",
    "            \n",
    "            if page_url_body_tag and 'error404' in page_url_body_tag.get('class', []):\n",
    "              print(f'Error 404 found on {page_url}. Stopping search.')\n",
    "              break # Stop searching further pages because if current page is not found then there are no more pages for this date.\n",
    "            else:\n",
    "              jobs_per_date = get_job_postings(page_url, headers)\n",
    "              jobs_from_all_dates.append(jobs_per_date)\n",
    "              print(f'Found {page_url}')\n",
    "          else:\n",
    "            print(f'Failed to retrieve {page_url}. Status code: {page_url_response.status_code}')\n",
    "            break  # Stop searching further pages on HTTP error\n",
    "      else:\n",
    "        print(f'Failed to retrieve {current_month_url}. Status code: {current_month_url_response.status_code}')\n",
    "        continue  # Stop searching further pages on HTTP error\n",
    "\n",
    "      print(f'Month {month} over.')\n",
    "\n",
    "      for jobs in jobs_from_all_dates:\n",
    "        merged_jobs.extend(jobs)\n",
    "        \n",
    "      df = pd.DataFrame(merged_jobs)\n",
    "      df.to_csv(f'job_postings_{month}-{year}.csv', index=False)\n",
    "        \n",
    "      print(f'{month}-{year} spreadsheet exported')\n",
    "\n",
    "      merged_jobs = []\n",
    "      jobs_from_all_dates = []\n",
    "        \n",
    "    print(f'Year {year} over.')h      \n",
    "\n",
    "  print(f'Scraping finished.')  \n",
    "\n",
    "aggregate_job_postings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: job_postings_1-2018.csv\n",
      "Reading file: job_postings_2-2018.csv\n",
      "Reading file: job_postings_3-2018.csv\n",
      "Reading file: job_postings_4-2018.csv\n",
      "Reading file: job_postings_5-2018.csv\n",
      "Reading file: job_postings_6-2018.csv\n",
      "Reading file: job_postings_7-2018.csv\n",
      "Reading file: job_postings_8-2018.csv\n",
      "Reading file: job_postings_9-2018.csv\n",
      "Reading file: job_postings_10-2018.csv\n",
      "Reading file: job_postings_11-2018.csv\n",
      "Reading file: job_postings_12-2018.csv\n",
      "Reading file: job_postings_1-2019.csv\n",
      "Reading file: job_postings_2-2019.csv\n",
      "Reading file: job_postings_3-2019.csv\n",
      "Reading file: job_postings_4-2019.csv\n",
      "Reading file: job_postings_5-2019.csv\n",
      "Reading file: job_postings_6-2019.csv\n",
      "Reading file: job_postings_7-2019.csv\n",
      "Reading file: job_postings_8-2019.csv\n",
      "Reading file: job_postings_9-2019.csv\n",
      "Reading file: job_postings_10-2019.csv\n",
      "Reading file: job_postings_11-2019.csv\n",
      "Reading file: job_postings_12-2019.csv\n",
      "Reading file: job_postings_1-2020.csv\n",
      "File not found: job_postings_1-2020.csv\n",
      "Reading file: job_postings_2-2020.csv\n",
      "File not found: job_postings_2-2020.csv\n",
      "Reading file: job_postings_3-2020.csv\n",
      "File not found: job_postings_3-2020.csv\n",
      "Reading file: job_postings_4-2020.csv\n",
      "File not found: job_postings_4-2020.csv\n",
      "Reading file: job_postings_5-2020.csv\n",
      "File not found: job_postings_5-2020.csv\n",
      "Reading file: job_postings_6-2020.csv\n",
      "Reading file: job_postings_7-2020.csv\n",
      "Reading file: job_postings_8-2020.csv\n",
      "Reading file: job_postings_9-2020.csv\n",
      "Reading file: job_postings_10-2020.csv\n",
      "Reading file: job_postings_11-2020.csv\n",
      "Reading file: job_postings_12-2020.csv\n",
      "Reading file: job_postings_1-2021.csv\n",
      "File not found: job_postings_1-2021.csv\n",
      "Reading file: job_postings_2-2021.csv\n",
      "File not found: job_postings_2-2021.csv\n",
      "Reading file: job_postings_3-2021.csv\n",
      "File not found: job_postings_3-2021.csv\n",
      "Reading file: job_postings_4-2021.csv\n",
      "File not found: job_postings_4-2021.csv\n",
      "Reading file: job_postings_5-2021.csv\n",
      "File not found: job_postings_5-2021.csv\n",
      "Reading file: job_postings_6-2021.csv\n",
      "Reading file: job_postings_7-2021.csv\n",
      "Reading file: job_postings_8-2021.csv\n",
      "Reading file: job_postings_9-2021.csv\n",
      "Reading file: job_postings_10-2021.csv\n",
      "Reading file: job_postings_11-2021.csv\n",
      "Reading file: job_postings_12-2021.csv\n",
      "Reading file: job_postings_1-2022.csv\n",
      "File not found: job_postings_1-2022.csv\n",
      "Reading file: job_postings_2-2022.csv\n",
      "File not found: job_postings_2-2022.csv\n",
      "Reading file: job_postings_3-2022.csv\n",
      "File not found: job_postings_3-2022.csv\n",
      "Reading file: job_postings_4-2022.csv\n",
      "File not found: job_postings_4-2022.csv\n",
      "Reading file: job_postings_5-2022.csv\n",
      "File not found: job_postings_5-2022.csv\n",
      "Reading file: job_postings_6-2022.csv\n",
      "Reading file: job_postings_7-2022.csv\n",
      "Reading file: job_postings_8-2022.csv\n",
      "Reading file: job_postings_9-2022.csv\n",
      "Reading file: job_postings_10-2022.csv\n",
      "Reading file: job_postings_11-2022.csv\n",
      "Reading file: job_postings_12-2022.csv\n",
      "Reading file: job_postings_1-2023.csv\n",
      "File not found: job_postings_1-2023.csv\n",
      "Reading file: job_postings_2-2023.csv\n",
      "File not found: job_postings_2-2023.csv\n",
      "Reading file: job_postings_3-2023.csv\n",
      "File not found: job_postings_3-2023.csv\n",
      "Reading file: job_postings_4-2023.csv\n",
      "File not found: job_postings_4-2023.csv\n",
      "Reading file: job_postings_5-2023.csv\n",
      "File not found: job_postings_5-2023.csv\n",
      "Reading file: job_postings_6-2023.csv\n",
      "Reading file: job_postings_7-2023.csv\n",
      "Reading file: job_postings_8-2023.csv\n",
      "Reading file: job_postings_9-2023.csv\n",
      "Reading file: job_postings_10-2023.csv\n",
      "Reading file: job_postings_11-2023.csv\n",
      "Reading file: job_postings_12-2023.csv\n",
      "Reading file: job_postings_1-2024.csv\n",
      "Reading file: job_postings_2-2024.csv\n",
      "Reading file: job_postings_3-2024.csv\n",
      "Reading file: job_postings_4-2024.csv\n",
      "Reading file: job_postings_5-2024.csv\n",
      "Reading file: job_postings_6-2024.csv\n",
      "Reading file: job_postings_7-2024.csv\n",
      "All files have been combined and saved to combined_job_postings.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_files():\n",
    "    first_year = 2018\n",
    "    first_month = 1\n",
    "    \n",
    "    date_today = date.today()\n",
    "    year_today = date_today.year\n",
    "    month_today = date_today.month\n",
    "\n",
    "    all_dataframes = []\n",
    "    \n",
    "    for year in range(first_year, year_today + 1):\n",
    "        if year == year_today:\n",
    "            last_month = month_today\n",
    "        else:\n",
    "            last_month = 12\n",
    "        \n",
    "        for month in range(first_month, last_month + 1):\n",
    "            file_name = f'job_postings_{month}-{year}.csv'\n",
    "            print(f'Reading file: {file_name}')\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(file_name)\n",
    "                all_dataframes.append(df)\n",
    "            except FileNotFoundError:\n",
    "                print(f'File not found: {file_name}')\n",
    "                continue\n",
    "\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        combined_df.to_csv('combined_job_postings.csv', index=False)\n",
    "        print('All files have been combined and saved to combined_job_postings.csv')\n",
    "    else:\n",
    "        print('No files were found to combine.')\n",
    "\n",
    "combine_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
