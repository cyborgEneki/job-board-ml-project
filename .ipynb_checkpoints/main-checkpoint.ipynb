{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from datetime import date\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Normalize Unicode characters\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://opportunitiesforyoungkenyans.co.ke'\n",
    "pagination_date_pattern = base_url + '/2024/06/13'\n",
    "pagination_pattern = pagination_date_pattern + '/page/3'\n",
    "\n",
    "headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_body(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_article = section.find('article', class_='post-content')\n",
    "\n",
    "      if post_article:\n",
    "        post_container = post_article.find('div', class_='clearfix').text\n",
    "        return post_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_url(url):\n",
    "  parsed_url =  urlparse(url)\n",
    "  \n",
    "  path_parts = parsed_url.path.split('/')\n",
    "  \n",
    "  year = path_parts[1]\n",
    "  month = path_parts[2]\n",
    "  day = path_parts[3]\n",
    "  \n",
    "  date = f'{year}-{month}-{day}'\n",
    "  return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_postings(url, headers):\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    section = soup.find('section', class_='main-content')\n",
    "\n",
    "    if section:\n",
    "      post_container = section.find('div', class_='col-lg-8')\n",
    "\n",
    "      job_posts = []\n",
    "      \n",
    "      for post in post_container.find_all('div', class_='post-classic'):\n",
    "        title_heading_element = post.find('h5')\n",
    "\n",
    "        title = clean_text(title_heading_element.find('a').text.strip())\n",
    "        link = title_heading_element.find('a').get(\"href\")\n",
    "\n",
    "        content = get_post_body(link, headers)\n",
    "        date = extract_date_from_url(link)\n",
    "\n",
    "        job_posts.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          'content': content,\n",
    "          'date': date\n",
    "        })\n",
    "\n",
    "      return job_posts\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "# all_jobs = get_job_postings(pagination_date_pattern, headers)\n",
    "# df = pd.DataFrame(all_jobs)\n",
    "# df.to_csv('job_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_job_postings():\n",
    "  headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "  }\n",
    "  \n",
    "  first_year = 2018\n",
    "  first_month = 1\n",
    "  first_day = 1\n",
    "\n",
    "  year_today = 2018\n",
    "  month_today = 1\n",
    "  day_today = 31\n",
    "  # date_today = date.today()\n",
    "  # year_today = date_today.year\n",
    "  # month_today = date_today.month\n",
    "  # day_today = date_today.day\n",
    "\n",
    "  base_url = 'https://opportunitiesforyoungkenyans.co.ke'\n",
    "\n",
    "  jobs_from_all_dates = []\n",
    "  merged_jobs = []\n",
    "  print('a')\n",
    "  for year in range(first_year, year_today + 1):\n",
    "    print('b')\n",
    "    \n",
    "    if year == year_today:\n",
    "      last_month = month_today\n",
    "    else:\n",
    "      last_month = 12\n",
    "    \n",
    "    for month in range(first_month, last_month + 1):\n",
    "      print('c')\n",
    "      \n",
    "      if year == year_today and month == month_today:\n",
    "        last_day = day_today\n",
    "      else:\n",
    "        last_day = 31\n",
    "\n",
    "      for day in range(first_day, last_day + 1):\n",
    "        print('d')\n",
    "          \n",
    "        current_date_url = f'{base_url}/{year}/{month}/{day}'\n",
    "        current_date_url_response = requests.get(current_date_url, headers=headers)\n",
    "\n",
    "        if current_date_url_response.status_code == 200:\n",
    "          print('e')\n",
    "            \n",
    "          soup = BeautifulSoup(current_date_url_response.content, 'html.parser')\n",
    "          date_url_body_tag = soup.body\n",
    "\n",
    "          if date_url_body_tag and 'error404' in date_url_body_tag.get('class', []):\n",
    "            print('f')\n",
    "              \n",
    "            print(f'No posts found at {page_url}. Skipping.')\n",
    "            continue\n",
    "          else:\n",
    "            for page_count in range(1, 1000):\n",
    "              print('g')\n",
    "                \n",
    "              page_url = f'{current_date_url}/page/{page_count}'\n",
    "              page_url_response = requests.get(page_url, headers=headers)\n",
    "              \n",
    "              if page_url_response.status_code == 200:\n",
    "                print('h')\n",
    "                  \n",
    "                page_url_soup = BeautifulSoup(page_url_response.content, 'html.parser')\n",
    "                page_url_body_tag = page_url_soup.body\n",
    "\n",
    "                if page_url_body_tag and 'error404' in page_url_body_tag.get('class', []):\n",
    "                  print('i')\n",
    "                    \n",
    "                  print(f'Error 404 found on {page_url}. Stopping search.')\n",
    "                  break # Stop searching further pages because if current page is not found then there are no more pages for this date.\n",
    "                else:\n",
    "                  print('j')\n",
    "                    \n",
    "                  jobs_per_date = get_job_postings(page_url, headers)\n",
    "                  jobs_from_all_dates.append(jobs_per_date)\n",
    "                  print(f'Found {page_url}')\n",
    "              else:\n",
    "                print('k')\n",
    "                \n",
    "                print(f'Failed to retrieve {page_url}. Status code: {page_url_response.status_code}')\n",
    "                break  # Stop searching further pages on HTTP error\n",
    "            \n",
    "            print(f'Day {day} over.')\n",
    "          # Continue to next day\n",
    "        else:\n",
    "          print(f'Failed to retrieve {current_date_url}. Status code: {current_date_url_response.status_code}')\n",
    "          continue  # Stop searching further pages on HTTP error\n",
    "      \n",
    "      print(f'Month {month} over.')\n",
    "    print(f'Year {year} over.')\n",
    "\n",
    "  for jobs in jobs_from_all_dates:\n",
    "    merged_jobs.extend(jobs)\n",
    "\n",
    "  count_of_merged_jobs = len(merged_jobs)\n",
    "\n",
    "  print(f'Count of merged jobs: {count_of_merged_jobs}')  \n",
    "  return merged_jobs\n",
    "\n",
    "aggregate_job_postings()\n",
    "# all_jobs = aggregate_job_postings()\n",
    "# df = pd.DataFrame(all_jobs)\n",
    "# df.to_csv('job_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
